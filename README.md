# Audio Spoofing Detection Using Speech Segment Analysis

## Overview

Audio spoofing attacks—where adversaries manipulate or synthesize speech to mimic legitimate speakers—pose significant threats to voice authentication systems and voice-controlled applications. Reliable detection of spoofed audio is critical to maintaining trust and security.

This project addresses the **binary classification problem** of distinguishing **bonafide (genuine)** vs **spoofed (fake)** audio samples by investigating how different **speech segment types** influence detection performance. We employ multiple machine learning models trained on speech segments derived from the ASVspoof 2019 Logical Access (LA) dataset, a benchmark dataset widely used for evaluating spoofing detection methods.

---

## Dataset

- **ASVspoof 2019 Logical Access (LA)** dataset is used for all experiments.  
- It contains **bonafide speech** and various **spoofed audio attacks** generated by text-to-speech (TTS) and voice conversion (VC) algorithms.  
- The dataset provides high-quality waveforms sampled at 16 kHz, with predefined training, development, and evaluation partitions to enable reproducible experiments.

---

## Problem Statement

- **Goal:** Develop a robust system to classify audio samples as bonafide or spoofed.  
- **Challenge:** Spoofed audio can closely mimic natural speech, often making detection difficult.  
- **Hypothesis:** Different speech segments—voiced, unvoiced, speech-only, and full audio—carry varied information that can affect spoof detection accuracy. By training models on these segment types separately, we can better understand which speech characteristics are most informative.

---

## Data Augmentation and Speech Segmentation

The original waveforms from ASVspoof 2019 LA are preprocessed to generate four versions, highlighting different acoustic characteristics:

### 1. Full Audio  
- The original, unmodified waveform containing all speech, silence, and background noise.

### 2. Speech-Only Audio (SAD)  
- Non-speech segments (silence, background noise) are removed using **Speech Activity Detection (SAD)** based on WebRTC Voice Activity Detection.  
- This yields audio focusing solely on active speech frames.

### 3. Voiced Segments  
- Voiced frames correspond to speech sounds produced with vocal fold vibration (vowels, sonorants).  
- Extracted using pitch detection algorithms (e.g., probabilistic YIN).  
- Characterized by quasi-periodic waveforms with identifiable fundamental frequency.

### 4. Unvoiced Segments  
- Speech frames without vocal fold vibration, including plosives and fricatives.  
- Extracted as the complement of voiced frames within the speech regions.

---

### Mathematical Representation of Segmentation

Given audio signal \( x(t) \) sampled at \( f_s \):

- **Speech mask \( m_s(t) \):** Indicates speech presence via VAD.  
- **Voiced mask \( m_v(t) \):** Indicates voiced frames via pitch detection.  
- **Unvoiced mask \( m_u(t) = m_s(t) \times (1 - m_v(t)) \).**

Segmented signals:

\[
\begin{aligned}
y_{\text{full}}(t) &= x(t) \\
y_{\text{speech}}(t) &= x(t) \times m_s(t) \\
y_{\text{voiced}}(t) &= x(t) \times m_v(t) \\
y_{\text{unvoiced}}(t) &= x(t) \times m_u(t)
\end{aligned}
\]

---

## Model Architectures

We implement and benchmark the following models on the segmented data:

### 1. YAMNet-like CNN  
- Processes time-frequency features (e.g., Mel spectrograms).  
- Uses convolutional layers with batch normalization and ReLU, followed by pooling and fully connected layers.

### 2. RawNet-like Model  
- Operates on raw waveforms.  
- Combines convolutional layers and bidirectional GRUs to capture spectral and temporal patterns.

### 3. LFCC_LCNN with Max-Feature-Map (MFM) Activation  
- Uses Linear Frequency Cepstral Coefficients (LFCC) as input features.  
- MFM activation enhances feature discrimination in convolutional layers.

### 4. Gaussian Mixture Model (GMM) on LFCC Features  
- Trains separate GMMs on bonafide and spoofed feature distributions.  
- Classification based on maximum likelihood.

---

## Installation

To set up the environment and run the project, follow these steps:

1. **Clone the repository**  
   ```bash
   git clone https://github.com/yourusername/audio-spoofing-segmentation.git
   cd audio-spoofing-segmentation
## References

- ASVspoof 2019 Challenge: [https://www.asvspoof.org/](https://www.asvspoof.org/)
- YAMNet: [https://github.com/tensorflow/models/tree/master/research/audioset/yamnet](https://github.com/tensorflow/models/tree/master/research/audioset/yamnet)
- RawNet: Jung et al., *RawNet: Advanced end-to-end deep neural network using raw waveforms for text-independent speaker verification* (ICASSP 2019).
- LFCC Features and MFM Activation: Lavrentyeva et al., *Audio replay attack detection with deep learning frameworks* (INTERSPEECH 2017).
